{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "KIatlSe71zxo"
      },
      "outputs": [],
      "source": [
        "FILE = 'SpamDetection.csv'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yFGv4Ceb1zxp"
      },
      "source": [
        "## Parse the CSV file into training and testing sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        },
        "id": "vrtBTgjX1zxs",
        "outputId": "9aad88e2-0f4d-4f64-858e-081e793ae026"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "\n",
        "all_set = []\n",
        "training_set = []\n",
        "testing_set = []\n",
        "with open(FILE) as csv_file:\n",
        "    csv_read=csv.reader(csv_file, delimiter=',')\n",
        "    for i, row in enumerate(csv_read):\n",
        "        if i == 0:\n",
        "            continue # skip the header row\n",
        "        all_set.append(row)\n",
        "        if i <= 20: # First 20 are training, rest are testing\n",
        "            training_set.append(row)\n",
        "        else:\n",
        "            testing_set.append(row)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CtrqWOlZ1zxt"
      },
      "source": [
        "## Seperate the training data into spam and ham"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "1NtC_AVP1zxu"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Probability of ham: 0.55\n",
            "Probability of spam: 0.45\n"
          ]
        }
      ],
      "source": [
        "prob_spam = 0\n",
        "prob_ham = 0\n",
        "spam_set: list[str] = []\n",
        "ham_set: list[str] = []\n",
        "for i in training_set:\n",
        "    if i[0] == 'spam':\n",
        "        prob_spam += 1\n",
        "        spam_set.append(i[1])\n",
        "    elif i[0] == 'ham':\n",
        "        prob_ham += 1\n",
        "        ham_set.append(i[1])\n",
        "training_sentences = spam_set + ham_set\n",
        "\n",
        "prob_spam = prob_spam / len(training_set)\n",
        "prob_ham = prob_ham / len(training_set)\n",
        "\n",
        "print(f\"Probability of ham: {prob_ham}\")\n",
        "print(f\"Probability of spam: {prob_spam}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TUQExwYy1zxv"
      },
      "source": [
        "## Define a basic tokenize function\n",
        "This will remove punctuation and lowercase all words so that it isn't case sensitive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "HdKI7Ig51zxw"
      },
      "outputs": [],
      "source": [
        "import string\n",
        "def tokenize(sentence: str) -> list[str]:\n",
        "    sentence = sentence.translate(str.maketrans('', '', string.punctuation)).lower() # remove punctuation and make lowercase\n",
        "    return sentence.split()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GdvWRI_u1zxw"
      },
      "source": [
        "# Word Counts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sP1YXgUG1zxx"
      },
      "source": [
        "## Function that counts how many of each word is in the set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "FpDyp_kG1zxx"
      },
      "outputs": [],
      "source": [
        "def get_word_counts(sentence_list: list[str]) -> tuple[dict[str, int], int]:\n",
        "    word_counts = {}\n",
        "    total_words = 0\n",
        "    for sentence in sentence_list:\n",
        "        sentence = tokenize(sentence)\n",
        "        for word in sentence:\n",
        "            total_words += 1\n",
        "            if word not in word_counts:\n",
        "                word_counts[word] = 1\n",
        "            else:\n",
        "                word_counts[word] += 1\n",
        "\n",
        "    return word_counts, total_words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vgLEsqD41zxy"
      },
      "source": [
        "## Function that gets the amount of unique words in a set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "7npCgjT01zxy"
      },
      "outputs": [],
      "source": [
        "def get_unique_word_count(sentence_list):\n",
        "    count = 0\n",
        "    for sentence in sentence_list:\n",
        "        sentence = tokenize(sentence)\n",
        "        for word in sentence:\n",
        "            count += 1\n",
        "\n",
        "    return count"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5qtDDoun1zxz"
      },
      "source": [
        "# Probabilities"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qDCRANEi1zxz"
      },
      "source": [
        "## Conditional Probability\n",
        "Performs laplace smoothing to avoid 0 prob"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "2SzC_FgW1zxz"
      },
      "outputs": [],
      "source": [
        "def get_conditional_prob(word_counts: int, word: str, total_words_in_list: int, unique_word_count: int) -> float:\n",
        "\n",
        "    prob = 1\n",
        "    if word in word_counts:\n",
        "        this_word_count = word_counts[word]\n",
        "    else:\n",
        "        this_word_count = 0\n",
        "    prob *= ((this_word_count + 1) / (total_words_in_list + unique_word_count))\n",
        "\n",
        "    conditional_prob = prob\n",
        "\n",
        "    return conditional_prob"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sMGQ2m6Z1zxz"
      },
      "source": [
        "## Posterior Probability\n",
        "Converts a conditional probability to a posterior probability by multiplying it by the prior"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "MGPXCx-V1zxz"
      },
      "outputs": [],
      "source": [
        "def get_posterior_prob(conditional_prob: int, prior_prob: int):\n",
        "    posterior_prob = conditional_prob * prior_prob\n",
        "    return posterior_prob"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nNDAGKW_1zx0"
      },
      "source": [
        "# Testing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qp2BFNce1zx0"
      },
      "source": [
        "## Define test\n",
        "Get the posterior probability of the sentence for both spam and ham, compare and decide. Then, check against the truth to see if it's correct. Finally, return the total accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "eHEr7k931zx0"
      },
      "outputs": [],
      "source": [
        "def test(test_set, spam_word_counts, spam_total_word_count, ham_word_counts, ham_total_word_count, unique_word_count, prob_spam_general, prob_ham_general):\n",
        "    correct = 0\n",
        "    for item in test_set:\n",
        "        sentence = item[1]\n",
        "        truth = item[0]\n",
        "\n",
        "        words = tokenize(sentence)\n",
        "        spam_posterior = 1\n",
        "        ham_posterior = 1\n",
        "        for word in words:\n",
        "            spam_conditional = get_conditional_prob(spam_word_counts, word, spam_total_word_count, unique_word_count)\n",
        "            spam_posterior *= get_posterior_prob(spam_conditional, prob_spam_general)\n",
        "\n",
        "            ham_conditional = get_conditional_prob(ham_word_counts, word, ham_total_word_count, unique_word_count)\n",
        "            ham_posterior *= get_posterior_prob(ham_conditional, prob_ham_general)\n",
        "\n",
        "\n",
        "\n",
        "        if spam_posterior > ham_posterior:\n",
        "            guess = \"spam\"\n",
        "        else:\n",
        "            guess = \"ham\"\n",
        "\n",
        "        print(f\"Sentence: {sentence}\")\n",
        "        print(f\"Spam Prob: {spam_posterior}\")\n",
        "        print(f\"Ham prob: {ham_posterior}\")\n",
        "        print(f\"Guess: {guess}\")\n",
        "        print(f\"Truth: {truth}\")\n",
        "        print(\"\\n\")\n",
        "\n",
        "        if guess == truth:\n",
        "            correct += 1\n",
        "\n",
        "    return correct / len(test_set)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6rfRdfcL1zx1"
      },
      "source": [
        "#### Initialize values from training set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "0t22WF681zx1"
      },
      "outputs": [],
      "source": [
        "spam_word_counts, spam_total_word_count = get_word_counts(spam_set)\n",
        "ham_word_counts, ham_total_word_count = get_word_counts(ham_set)\n",
        "unique_word_count = get_unique_word_count(ham_set + spam_set)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tmwhVodX1zx1"
      },
      "source": [
        "## Run The Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "GPnYcd3v1zx1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sentence: Tell where you reached\n",
            "Spam Prob: 8.294857291727621e-12\n",
            "Ham prob: 1.2050870120331484e-10\n",
            "Guess: ham\n",
            "Truth: ham\n",
            "\n",
            "\n",
            "Sentence: Your gonna have to pick up a burger for yourself on your way home\n",
            "Spam Prob: 8.970648332408384e-38\n",
            "Ham prob: 9.005455843645994e-37\n",
            "Guess: ham\n",
            "Truth: ham\n",
            "\n",
            "\n",
            "Sentence: As a valued customer I am pleased to advise you that for your recent review you are awarded a Bonus Prize\n",
            "Spam Prob: 9.107459206397537e-56\n",
            "Ham prob: 8.881175033547704e-54\n",
            "Guess: ham\n",
            "Truth: spam\n",
            "\n",
            "\n",
            "Sentence: Urgent you are awarded a complimentary trip to EuroDisinc To claim text immediately\n",
            "Spam Prob: 4.992123079612254e-34\n",
            "Ham prob: 1.3590051545865772e-34\n",
            "Guess: spam\n",
            "Truth: spam\n",
            "\n",
            "\n",
            "Sentence: Finished class where are you\n",
            "Spam Prob: 1.7988847138686408e-14\n",
            "Ham prob: 1.996379086199493e-13\n",
            "Guess: ham\n",
            "Truth: ham\n",
            "\n",
            "\n",
            "Sentence: where are you how did you perform \n",
            "Spam Prob: 1.2690636825956735e-19\n",
            "Ham prob: 7.01297341936823e-17\n",
            "Guess: ham\n",
            "Truth: ham\n",
            "\n",
            "\n",
            "Sentence: you can call me now\n",
            "Spam Prob: 1.888828949562073e-13\n",
            "Ham prob: 7.985516344797972e-13\n",
            "Guess: ham\n",
            "Truth: ham\n",
            "\n",
            "\n",
            "Sentence: I am waiting Call me once you are free\n",
            "Spam Prob: 8.704203658389546e-25\n",
            "Ham prob: 1.4434863047241683e-22\n",
            "Guess: ham\n",
            "Truth: ham\n",
            "\n",
            "\n",
            "Sentence: I am on the way to homei\n",
            "Spam Prob: 1.3571931049981507e-19\n",
            "Ham prob: 6.574662580657716e-18\n",
            "Guess: ham\n",
            "Truth: ham\n",
            "\n",
            "\n",
            "Sentence: Please call our customer service representative between 10am-9pm as you have WON a guaranteed cash prize\n",
            "Spam Prob: 9.941569873900718e-43\n",
            "Ham prob: 3.0893344379433172e-43\n",
            "Guess: spam\n",
            "Truth: spam\n",
            "\n",
            "\n",
            "Accuracy: 0.9\n"
          ]
        }
      ],
      "source": [
        "accuracy = test(testing_set, spam_word_counts, spam_total_word_count, ham_word_counts, ham_total_word_count, unique_word_count, prob_spam, prob_ham)\n",
        "print(f\"Accuracy: {accuracy}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
